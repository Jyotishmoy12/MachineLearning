{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfG/Nlkm5aYdSBHyzlLZlR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jyotishmoy12/MachineLearning/blob/main/svmfromskratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bD5I6aW0GZpk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Support_Vector_Machine:\n",
        "    # The __init__ method is a special method that is automatically called when an instance of the class is created.\n",
        "    # It is used to initialize the object's attributes (properties).\n",
        "    def __init__(self, visualization=True):\n",
        "        # The 'self' keyword is a reference to the current instance of the class.\n",
        "        # It allows access to the instance's attributes and methods.\n",
        "\n",
        "        # This attribute is used to determine whether or not to enable visualization.\n",
        "        self.visualization = visualization\n",
        "\n",
        "        # This attribute is a dictionary that maps class labels (1 and -1) to colors ('r' for red, 'b' for blue).\n",
        "        self.colors = {1: 'r', -1: 'b'}\n",
        "\n",
        "        # If visualization is enabled, create a new figure and axes for plotting.\n",
        "        if self.visualization:\n",
        "            # Create a new figure for plotting.\n",
        "            self.fig = plt.figure()\n",
        "            # Add a subplot to the figure with a 1x1 grid and select the first (and only) subplot.\n",
        "            self.ax = self.fig.add_subplot(1, 1, 1)\n",
        "\n",
        "\n",
        "def fit(self, data):\n",
        "    # The 'fit' method is used to train the Support Vector Machine (SVM) model on the provided data.\n",
        "\n",
        "    self.data = data\n",
        "    # The 'data' parameter is a dictionary where keys are class labels (e.g., 1 or -1)\n",
        "    # and values are lists of feature sets (each feature set is a list of features).\n",
        "    # This line assigns the provided data to the instance attribute 'self.data'\n",
        "    # so it can be accessed throughout the class.\n",
        "\n",
        "    # 'opt_dict' will be a dictionary that stores the optimal values of the weights (w) and bias (b).\n",
        "    # The keys are the magnitudes of the weight vector ||w||, and the values are lists [w, b].\n",
        "    opt_dict = {}\n",
        "\n",
        "    # 'transforms' is a list of possible transformations (multiplying by +1 or -1) to apply to the weight vector.\n",
        "    # These transformations correspond to the four quadrants in a 2D space, helping to explore different orientations of the weight vector.\n",
        "    transforms = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
        "\n",
        "    # 'all_data' is a list that will contain all feature values from the dataset.\n",
        "    # It is used to determine the maximum and minimum feature values, which are important for setting step sizes.\n",
        "    all_data = []\n",
        "\n",
        "    # This nested loop goes through each class label (yi) and its associated feature sets in 'self.data'.\n",
        "    for yi in self.data:\n",
        "        for featureset in self.data[yi]:\n",
        "            # For each feature set, it appends each individual feature value to 'all_data'.\n",
        "            for feature in featureset:\n",
        "                all_data.append(feature)\n",
        "\n",
        "    # Find the maximum value among all features in the dataset.\n",
        "    # This value is crucial for determining the step sizes for gradient descent.\n",
        "    self.max_feature_value = max(all_data)\n",
        "\n",
        "    # Find the minimum value among all features in the dataset.\n",
        "    # This value is also crucial for determining the step sizes for gradient descent.\n",
        "    self.min_feature_value = min(all_data)\n",
        "\n",
        "    # Clear the 'all_data' list to free up memory, as it's no longer needed after determining min/max feature values.\n",
        "    all_data = None\n",
        "\n",
        "    # 'step_sizes' is a list of step sizes that will be used to adjust the weight vector during the optimization process.\n",
        "    # The step sizes are fractions of the maximum feature value, starting with larger steps and gradually reducing to finer steps.\n",
        "    # This approach allows for faster convergence initially and finer adjustments later.\n",
        "    step_sizes = [\n",
        "        self.max_feature_value * 0.1,   # Large step size for initial, rough adjustments\n",
        "        self.max_feature_value * 0.01,  # Medium step size for finer adjustments\n",
        "        self.max_feature_value * 0.001  # Small step size for the final, precise adjustments\n",
        "    ]\n",
        "\n",
        "    # 'b_range_multiple' is a multiplier used to define the range in which the bias term (b) will be optimized.\n",
        "    # The range is determined relative to the magnitude of the feature values.\n",
        "    b_range_multiple = 5\n",
        "\n",
        "    # 'b_multiple' is a multiplier used to control the granularity of steps taken when optimizing the bias term (b).\n",
        "    # A larger value means fewer steps, while a smaller value means more fine-grained steps.\n",
        "    b_multiple = 5\n",
        "\n",
        "    # 'latest_optimum' is a starting point for the optimization process.\n",
        "    # It is initialized to a value ten times the maximum feature value.\n",
        "    # This serves as a large initial guess for the weight vector (w) and will be refined during optimization.\n",
        "    latest_optimum = self.max_feature_value * 10\n",
        "\n",
        "    for step in step_sizes:\n",
        "        w=np.array([latest_optimum, latest_optimum])\n",
        "        optimzed=False\n",
        "        while not optimzed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self,features):\n",
        "      #sign(x.w+b)\n",
        "      classification=np.sign(np.dot(np.array(features), self.w)+self.b)\n",
        "      return classification\n",
        "\n"
      ],
      "metadata": {
        "id": "aqOPvWj1Hxfh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict={-1:np.array([[1,7],[2,8], [3,8],],),\n",
        "                       1:np.array([[5,1], [6,-1], [7,3], ])}\n",
        ""
      ],
      "metadata": {
        "id": "uLxWysxWG74O"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}